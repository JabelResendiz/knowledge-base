{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Laboratorio #1: Modelo Booleano de Recuperación de Información\n",
    "\n",
    "Un Modelo de Recuperación de Información (MRI) es un sistema diseñado para facilitar la búsqueda y extracción de información relevante de grandes bases de datos o colecciones de documentos. Estos modelos son cruciales en áreas como los motores de búsqueda en internet, los sistemas de gestión documental y las bibliotecas digitales. Permiten a los usuarios encontrar información específica basada en criterios de búsqueda variados. Algunos de los modelos más significativos incluyen el modelo booleano, que utiliza operadores lógicos para la búsqueda; y el modelo vectorial, que emplea conceptos de álgebra lineal para relacionar documentos y consultas.\n",
    "\n",
    "Durante el laboratorio se estará trabajando con el Modelo Booleano. Este de define como:\n",
    "- **D**: Vectores binarios asociados a los términos indexados.\n",
    "- **Q**: Expresión booleana sobre los términos indexados, utilizando los operadores: _and_, _or_, _not_.\n",
    "- **F**: Álgebra booleana y teoría de conjuntos.\n",
    "- **R**: $sim(d_j, q) = \n",
    "\t\t \t\t\\left\\{\n",
    "\t\t \t\t\t\\begin{array}{ll} \n",
    "\t\t \t\t\t\t1 & si \\ \\exists \\overrightarrow{q_{cc}} : \\overrightarrow{q_{cc}} \\in \\overrightarrow{q_{fnd}} \\ \\wedge \\ (\\forall t_i :\\  g_i(\\overrightarrow{d_j}) = g_i(\\overrightarrow{q_{cc}}))\\\\ \\\\\n",
    "\t\t \t\t\t\t0 & en \\ otro \\ caso\n",
    "\t\t \t\t\t\\end{array} \n",
    "\t\t \t\t\\right.\n",
    "\t\t \t$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configurando el entorno\n",
    "\n",
    "# Fuente del corpus\n",
    "import ir_datasets\n",
    "\n",
    "# Trabajo con los operadores lógicos\n",
    "from sympy import sympify, to_dnf, Not, And, Or\n",
    "\n",
    "# Facilita el trabajo con los términos indexados del corpus\n",
    "from gensim.corpora import Dictionary\n",
    "\n",
    "# Funciones útiles auxiliares\n",
    "from teacher_help import tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cranfield\n",
    "\n",
    "El corpus Cranfield es un conjunto clásico de datos en el campo de la Recuperación de Información compuesto por aproximadamente 1,400 resúmenes de artículos de investigación en aerodinámica. Cada documento en este corpus incluye un título, un resumen conciso del contenido, y en algunos casos, palabras clave y referencias bibliográficas. Acompañando a estos documentos, hay alrededor de 225 consultas de prueba y sus correspondientes juicios de relevancia, proporcionados por expertos, que indican la pertinencia de cada documento para una consulta específica. Este diseño estructurado y su enfoque específico en temas de aerodinámica hacen del corpus una herramienta esencial para evaluar la eficacia de Sistemas de Recuperación de Información (SRI), sirviendo como un modelo estándar para pruebas comparativas y consistentes en esta disciplina.\n",
    "\n",
    "La variable **dataset**, instancia de la clase **ir_datasets.datasets.base.Dataset**, tiene 3 funciones:\n",
    "1. _docs_iter()_\n",
    "2. _queries_iter()_\n",
    "3. _qrels_iter()_\n",
    "\n",
    "Pero durante esta clase, solo se trabajará con la primera de ellas. La función **docs_iter()** devuelve un objeto iterable de tuplas de dimensión 5, referentes a los documentos. Los campos de cada tupla son:\n",
    "  - Identificador (str)\n",
    "  - Título (str)\n",
    "  - Texto (str)\n",
    "  - Autor (str)\n",
    "  - Referencia bibliográfica (str)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejercicio #1: Cargue en memoria el corpus Cranfield, tokenice cada documento y almacénelos en una lista llamada *tokenized_docs*.\n",
    "\n",
    "###### Ayuda #1: Visite el sitio del catálogo de datasets contenidos en ir_datasets y consulte el apartado referente a Cranfield. \n",
    "###### Ayuda #2: Consulte la función `tokenize` hecha por los profesores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = ir_datasets.load(\"cranfield\")\n",
    "\n",
    "tokenized_docs= []\n",
    "\n",
    "for doc in dataset.docs_iter():\n",
    "    tokenized_docs.append(tokenize(doc.text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creando mi primera estructura de datos para un MRI\n",
    "\n",
    "Los términos invertidos, también conocidos como **índices invertidos**, son estructuras de datos utilizadas en Sistemas de Recuperación de Información (SRI) para asociar términos (o palabras) con los documentos que los contienen. Básicamente, son diccionarios que asignan términos a una lista de documentos en los que aparecen esos términos. Esto permite una búsqueda eficiente de documentos que contienen ciertos términos.\n",
    " \n",
    "\n",
    "### Ejercicio #2: Implemente una función que dado un corpus tokenizado, devuelva los índices invertidos.\n",
    "Considere que cada documento posee un identificador único."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 24\u001b[39m\n\u001b[32m     20\u001b[39m             inverted_index[token].append(doc_id)\n\u001b[32m     22\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m inverted_index\n\u001b[32m---> \u001b[39m\u001b[32m24\u001b[39m inverted_index = \u001b[43mbuild_inverted_index\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokenized_docs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     25\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mlist\u001b[39m(inverted_index.keys())[:\u001b[32m15\u001b[39m])\n\u001b[32m     26\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mlist\u001b[39m(inverted_index.items())[\u001b[32m0\u001b[39m])\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 15\u001b[39m, in \u001b[36mbuild_inverted_index\u001b[39m\u001b[34m(data)\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[33;03mBuild an inverted index structure\u001b[39;00m\n\u001b[32m      4\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m     11\u001b[39m \n\u001b[32m     12\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     13\u001b[39m inverted_index = {}\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m doc_id, tokens \u001b[38;5;129;01min\u001b[39;00m data:\n\u001b[32m     16\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mset\u001b[39m(tokens):\n\u001b[32m     17\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m token \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m inverted_index:\n",
      "\u001b[31mValueError\u001b[39m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "def build_inverted_index(data):\n",
    "    \"\"\"\n",
    "    Build an inverted index structure\n",
    "    \n",
    "    Args:\n",
    "    - data : [(str, [str])]\n",
    "        List of tuples of size 2. The first position contains the document identifier and the second the list of document tokens.\n",
    "\n",
    "    Return:\n",
    "    - dict\n",
    "    \n",
    "    \"\"\"\n",
    "    inverted_index = {}\n",
    "\n",
    "    for doc_id, tokens in data:\n",
    "        for token in set(tokens):\n",
    "            if token not in inverted_index:\n",
    "                inverted_index[token]=[]\n",
    "            \n",
    "            inverted_index[token].append(doc_id)\n",
    "    \n",
    "    return inverted_index\n",
    "\n",
    "inverted_index = build_inverted_index(tokenized_docs)\n",
    "print(list(inverted_index.keys())[:15])\n",
    "print(list(inverted_index.items())[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Representación de la consulta en FND\n",
    "\n",
    "La Forma Normal Disyuntiva es una forma estándar de expresar una fórmula lógica booleana como una disyunción de cláusulas, donde cada cláusula es una conjunción de literales (variables booleanas o sus negaciones). Esta representación es útil en la lógica booleana y en la teoría de la computación porque permite realizar operaciones y simplificaciones sobre las expresiones booleanas de manera sistemática. Además, muchas funciones lógicas pueden ser representadas eficientemente en DNF.\n",
    "\n",
    "### Ejercicio #3: Implemente una función que dada una query booleana, devuelva la FND de la misma.\n",
    "###### Ayuda #1: Considere usar las funciones `sympify` y `to_dnf`.\n",
    "###### Ayuda #2: Considere usar los símbolos `&`, `|` y `~` para sustituir los operadores lógicos `and`, `or` y `not`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_to_dnf(query):\n",
    "    \"\"\"\n",
    "    Transforms a boolean expression into its disjunctive normal form\n",
    "    \n",
    "    Args:\n",
    "    - query : str\n",
    "        Boolean query.\n",
    "\n",
    "    Return:\n",
    "    - instance of class Or\n",
    "    \n",
    "    \"\"\"\n",
    "    raise Exception('Not Implemented')\n",
    "\n",
    "query = \"A AND (B OR NOT C)\"\n",
    "query_dnf = query_to_dnf(query)\n",
    "print(query_dnf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejercicio #4: Implemente la función de similitud del modelo booleano, de forma que dada la consulta y el conjunto de documentos tokenizados la función devuelva aquellos documentos que satifacen a la consulta.\n",
    "###### Ayuda: Considere usar las clase `Dictionary` de *gensim* y las funciones `Dictionary.token2id` y `Dictionary.doc2bow`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_matching_docs(query_dnf):\n",
    "    \"\"\"\n",
    "    Similarity function\n",
    "\n",
    "    Args:\n",
    "    - query_dnf : Instance of class Or\n",
    "        Query in FND\n",
    "\n",
    "    Return:\n",
    "    - [boolean] \n",
    "        Each position matches a document in the corpus. If the value is True then the document satisfies the query, False otherwise.\n",
    "\n",
    "    \"\"\"\n",
    "    raise Exception('Not Implemented')\n",
    "\n",
    "query = \"experimental AND (aerodynamic OR NOT propeller)\"\n",
    "query = ' '.join(tokenize(query, is_query=True))\n",
    "query_dnf = query_to_dnf(query)\n",
    "get_matching_docs(query_dnf) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejercicio #5: ¿Cómo pudiese implementarse la función de similitud usando solamente la consulta en su FND y la estructura de los índices invertidos creada a partir del corpus procesado?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "clean_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
