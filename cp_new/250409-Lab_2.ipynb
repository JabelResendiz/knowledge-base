{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Laboratorio#2: Modelo Vectorial de Recuperación de Información\n",
    "\n",
    "El Modelo  Vectorial de Recuperación de Información (MVRI) es un modelo algebraico para representar documentos de texto (o más generalmente, elementos) como vectores, de modo que la distancia entre vectores representa la relevancia entre los documentos. Es fundamental para una serie de operaciones de recuperación de información (RI), incluida la puntuación de documentos en una consulta, la clasificación de documentos y la agrupación de documentos.\n",
    "\n",
    "Durante la clase se estará trabajando con el Modelo Vectorial. Este de define como:\n",
    "- **D**:  Vectores de pesos no binarios asociados a los términos de los documentos.\n",
    "- **Q**: Vectores de pesos no binarios asociados a los términos de la consulta.\n",
    "- **F**: Espacio n-dimensional y operaciones entre vectores del Álgebra Lineal.\n",
    "- **R**: $sim(d_j, q) = \n",
    "\t\t \t\t\\displaystyle{\\frac{\\sum_{i=1}^{n} w_{i,j} \\times w_{i,q}}{\\sqrt{\\sum_{i=1}^{n} w_{i,j}^{2}}\\times \\sqrt{\\sum_{i=1}^{n} w_{i,q}^{2}}}}\n",
    "\t\t \t$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configurando el entorno\n",
    "\n",
    "# Fuente del corpus\n",
    "import ir_datasets\n",
    "\n",
    "# Facilita el trabajo con los términos indexados del corpus\n",
    "from gensim.corpora import Dictionary\n",
    "\n",
    "# Para realizar el proceso de reduccion de dimensiones\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Funciones útiles auxiliares\n",
    "from teacher_help import tokenize, cosine_similarity\n",
    "\n",
    "# Facilita la creación de vectores y matrices de gran dimensión\n",
    "import numpy as np\n",
    "\n",
    "# Facilita el trabajo con funciones matemáticas\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cranfield\n",
    "\n",
    "El corpus Cranfield es un conjunto clásico de datos en el campo de la Recuperación de Información compuesto por aproximadamente 1,400 resúmenes de artículos de investigación en aerodinámica. Cada documento en este corpus incluye un título, un resumen conciso del contenido, y en algunos casos, palabras clave y referencias bibliográficas. Acompañando a estos documentos, hay alrededor de 225 consultas de prueba y sus correspondientes juicios de relevancia, proporcionados por expertos, que indican la pertinencia de cada documento para una consulta específica. Este diseño estructurado y su enfoque específico en temas de aerodinámica hacen del corpus una herramienta esencial para evaluar la eficacia de Sistemas de Recuperación de Información (SRI), sirviendo como un modelo estándar para pruebas comparativas y consistentes en esta disciplina.\n",
    "\n",
    "La variable **dataset**, instancia de la clase **ir_datasets.datasets.base.Dataset**, tiene 3 funciones:\n",
    "1. _docs_iter()_\n",
    "2. _queries_iter()_\n",
    "3. _qrels_iter()_\n",
    "\n",
    "Pero durante esta clase, solo se trabajará con la primera de ellas. La función **docs_iter()** devuelve un objeto iterable de tuplas de dimensión 5, referentes a los documentos. Los campos de cada tupla son:\n",
    "  - Identificador (str)\n",
    "  - Título (str)\n",
    "  - Texto (str)\n",
    "  - Autor (str)\n",
    "  - Referencia bibliográfica (str)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "En el laboratorio anterior se vio, entre otras cosas, el proceso de la tokenización de documentos y la representación de los documentos según BoW (Bag Of Words, o bolsa de palabras). \n",
    "\n",
    "Nuestro primer objetivo es ejercitar los contenidos antes mencionados y preparar los datos para su posterior uso. Además veremos el proceso de generación del vocabulario.\n",
    "\n",
    "### Ejercicio 1: Prepare los datos para poder trabajar.\n",
    "Para ello, \n",
    "\n",
    "a) Cree el vocabulario del conjunto de datos.\n",
    "\n",
    "b) Convierta a la representación **bolsa de palabras** cada documento.\n",
    "\n",
    "###### Ayuda: Considere usar la clase `Dictionary`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En el caso de la representación de los documentos en el Modelo Vectorial, tanto los documentos como las consultas se representan como vectores no binarios, donde cada dimensión indica el peso de un término o *token* independiente en el documento. Se han desarrollado varias formas diferentes de calcular estos valores, y uno de los esquemas más conocidos es la ponderación *tf-idf*.\n",
    "\n",
    "El *tf–idf* es el producto de dos estadísticas: la *frecuencia de los términos* ($tf_{i,j}$) y la *frecuencia inversa de los documentos* ($idf_i$). Siendo $w_{i,j}$ el peso de una palabra en documento se cumple que:\n",
    "$$\n",
    "w_{i,j} = tf_{i,j} * idf_i\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "La *frecuencia nromalizada del término* $t_i$ en el documento $d_j$ se define como:\n",
    "\n",
    "$$\n",
    "tf_{i,j} = \\frac{freq_{i,j}}{max\\_freq_{i,j}} = \\alpha + (1 - \\alpha) \\cdot \\frac{freq_{i,j}}{max\\_freq_{i,j}}\n",
    "$$\n",
    "\n",
    "Donde:\n",
    "- $freq_{i,j}$: Frecuencia del término $t_i$ en el documento $d_j$.\n",
    "- $max\\_freq_{i,j}$: Mayor frecuencia entre todos los términos del documento $d_j$.\n",
    "- $\\alpha \\in [0, 1]$: Término de suavizado que amortigua la contribución de la frecuencia del término. \n",
    "Si el término no aparece en el documento ($t_i \\notin d_j$), entonces $tf_{i,j} = 0$\n",
    "\n",
    "La *frecuencia inversa de los documentos* está dada por:\n",
    "\n",
    "$$\n",
    "idf_i = \\log\\left(\\frac{N}{n_i}\\right)\n",
    "$$\n",
    "\n",
    "Donde:\n",
    "- $N$: Número total de documentos en el corpus.\n",
    "- $n_i$: Número de documentos que contienen el término $t_i$.\n",
    "\n",
    "---\n",
    "\n",
    "### Ejercicio 2:\n",
    "Obtenga la representación **tf-idf** de cada elemento del corpus y de la consulta definida. Para esto:\n",
    "- 1. Implemente la clase TfidfModel.\n",
    "- 2. Cree un objeto de la clase implementada pasandole el corpues previamente construído.\n",
    "###### Ayuda: Vea la clase TfidfModel de gensim.models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TfidfModel:\n",
    "    def __init__(self, corpus_bow, dictionary, alpha=0.0):\n",
    "        \"\"\"\n",
    "        Inicializa el modelo tf-idf y calcula los IDF globales.\n",
    "\n",
    "        Args:\n",
    "            corpus_bow: Corpus en formato BoW (lista de [(term_id, freq), ...]).\n",
    "            dictionary: Objeto Dictionary de gensim.\n",
    "            alpha: Parámetro de suavizado (opcional).\n",
    "        \"\"\"\n",
    "        self.dictionary = dictionary\n",
    "        self.alpha = alpha\n",
    "        self.num_docs = len(corpus_bow)\n",
    "        self.corpus_bow = corpus_bow\n",
    "        self.idf = self._compute_idf(corpus_bow)\n",
    "        self.corpus_tfidf = self.corpus2dense(self.transform())\n",
    "\n",
    "    def _compute_idf(self, corpus_bow):\n",
    "        \"\"\"\n",
    "        Calcula la frecuencia inversa de documento (idf).\n",
    "        \n",
    "        Args:\n",
    "            - corpus_bow : [[(int, int)]]\n",
    "                Corpus represenatdo en bosla de palabras (bow).\n",
    "        \n",
    "        Return:\n",
    "        - {int: float}\n",
    "\n",
    "        \"\"\"\n",
    "        \n",
    "        # TODO\n",
    "\n",
    "    def _compute_tf(self, bow_doc):\n",
    "        \"\"\"\n",
    "        Calcula tf normalizado con suavizado para un documento.\n",
    "        \n",
    "        Args:\n",
    "            - bow_doc : [(int, int)]\n",
    "                Documento representado en bolsa de palabras.\n",
    "            \n",
    "        Return:\n",
    "        - {int: float}\n",
    "        \"\"\"\n",
    "        \n",
    "        # TODO\n",
    "        \n",
    "\n",
    "    def get_document_tfidf(self, bow_doc):\n",
    "        \"\"\"\n",
    "        Devuelve tf-idf de un solo documento.\n",
    "        \n",
    "        Args:\n",
    "            - bow_doc : [(int, int)]\n",
    "                Documento representado en bolsa de palabras.\n",
    "            \n",
    "        Return:\n",
    "        - [(int, float)]\n",
    "        \"\"\"\n",
    "        \n",
    "        # TODO\n",
    "\n",
    "    def transform(self, corpus=None):\n",
    "        \"\"\"\n",
    "        Devuelve el corpus transformado a tf-idf en una matriz dispersa.\n",
    "        \n",
    "        Args:\n",
    "            - corpus : [(int, int)]\n",
    "                Corpus compuesto por los documentos representados en bolsa de palabras.\n",
    "            \n",
    "        Return:\n",
    "        - [[(int, float)]]\n",
    "        \"\"\"\n",
    "        \n",
    "        # TODO\n",
    "\n",
    "    def corpus2dense(self, tfidf_corpus):\n",
    "        \"\"\"\n",
    "        Convierte el corpus tf-idf a una matriz densa [docs x terms].\n",
    "        \n",
    "        Args:\n",
    "            - corpus : [(int, int)]\n",
    "                Corpus compuesto por los documentos representados en bolsa de palabras.\n",
    "            \n",
    "        Return:\n",
    "        - numpy.array(num_docs, num_terms)\n",
    "        \"\"\"\n",
    "        \n",
    "        # TODO\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejercicio 2.1:\n",
    "Obtenga el tf-idf asociado a la query dada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_text = 'what similarity laws must be obeyed when constructing aeroelastic models of heated high speed aircraft .'\n",
    "\n",
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejercicio 3: Obtenga el ranking (decreciente) de los documentos que _satisfacen_ a la consulta previamente definida.\n",
    "\n",
    "###### Ayuda: Considere usar la función `cosine_similarity`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_documents(corpus_matriz, vector_query):\n",
    "    \"\"\"\n",
    "    Gets the similarity between the corpus and a query\n",
    "    \n",
    "    Args:\n",
    "    - corpus_matriz : [[float]]\n",
    "        tf-idf representation of the query. Each row is considered a document.\n",
    "    - vector_query : [float]\n",
    "        tf-idf representation of the query.\n",
    "        \n",
    "    Return:\n",
    "    - [(int, float)]\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    # TODO\n",
    "\n",
    "retrived_docs = retrieve_documents(tfidf.corpus_tfidf, query_vector[0])\n",
    "print(retrived_docs[:10])\n",
    "print(len(retrived_docs)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uno de los problemas más comunes cuando se trabaja con datos muchas dimensiones es el conocido \"maldición de la dimensionalidad\". Este fenómeno entre otras cosas afecta nociones como distancia o similitud, haciendo que todos los objetos parezcan distantes y diferentes.\n",
    "\n",
    "Debido a esto, una de las ideas básicas para evitar estos problemas consiste en aplicar técnicas de reducción de dimensiones. Este proceso consiste en la transformación de datos de un espacio de alta dimensión a un espacio de baja dimensión, pero en este nuevo espacio se intenta conservar algunas propiedades significativas de los datos originales, idealmente cercanas a su dimensión intrínseca.\n",
    "\n",
    "Una de las principales técnicas lineales para la reducción de la dimensionalidad es el Análisis de Componentes Principales (PCA por sus siglas en inglés). Esta técnica realiza un mapeo lineal de los datos a un espacio de dimensiones inferiores de tal manera que se maximiza la varianza de los datos en la representación de dimensiones bajas.\n",
    "\n",
    "### Ejercicio 4: Reduzca las dimensiones del corpus (vectores de tf-idf) y con los nuevos vectores recupere los documentos asociados a la consulta dada.\n",
    "###### Ayuda: Considere usar la clase `PCA` y de ahí, las funciones `fit` y `transform`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Índice de varianza\n",
    "variance = 0.90\n",
    "\n",
    "# TODO"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "clean_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
