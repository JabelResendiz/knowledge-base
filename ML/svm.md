# üß† M√°quinas de Soporte Vectorial (SVM)

## 1. Introducci√≥n

Las **M√°quinas de Soporte Vectorial (SVM)** son modelos de **aprendizaje supervisado** utilizados principalmente para **clasificaci√≥n**.  
Buscan encontrar el **hiperplano que mejor separa las clases** en el espacio de caracter√≠sticas.

Fueron desarrolladas en 1963 por **Vladimir Vapnik** y **Alexei Chervonenkis** en la Uni√≥n Sovi√©tica.

---

## 2. Idea principal: el hiperplano separador

En un problema de clasificaci√≥n binaria, los datos se dividen seg√∫n una ecuaci√≥n:

\[
w^T x + b = 0
\]

- \( w \): vector normal al hiperplano.  
- \( b \): sesgo (distancia del hiperplano al origen).  
- La predicci√≥n se realiza con el signo de \( f(x) = w^T x + b \):

\[
\text{sign}(f(x)) =
\begin{cases}
+1, & \text{si pertenece a la clase positiva}\\
-1, & \text{si pertenece a la clase negativa}
\end{cases}
\]

---

## 3. Margen y vectores de soporte

SVM no busca **cualquier** hiperplano que separe las clases, sino el que **maximiza el margen**, es decir, la distancia entre el hiperplano y los puntos m√°s cercanos.

\[
\text{Margen} = \frac{2}{||w||}
\]

Los puntos m√°s cercanos a la frontera son los **vectores de soporte**.  
Ellos son los que definen la posici√≥n y orientaci√≥n del hiperplano.

---

## 4. Formulaci√≥n matem√°tica

El problema se plantea como:

\[
\min_{w,b} \frac{1}{2} ||w||^2
\]

sujeto a:

\[
y_i (w^T x_i + b) \ge 1, \quad \forall i
\]

Esto es un **problema de optimizaci√≥n cuadr√°tica convexa**, cuya soluci√≥n da el hiperplano con margen m√°ximo.

- El vector w define la orientaci√≥n del hiperplano
- Su norma || w || est√° inversamente relacionado con el margen: cuanto m√°s peque√±o || w || , m√°s ancho el margen.
- Las restricciones garantizan que no hay puntos mal clasificados. 
- LOs puntos que tocan las lineas w^T x + b = +1 o -1 son los vectores de soporte.


---

## 5. SVM con datos no linealmente separables

Cuando no es posible separar las clases perfectamente, se introduce un t√©rmino de error \(\xi_i\):

\[
\min_{w,b} \frac{1}{2} ||w||^2 + C \sum_i \xi_i
\]

sujeto a:

\[
y_i (w^T x_i + b) \ge 1 - \xi_i, \quad \xi_i \ge 0
\]

- \( C \): par√°metro de penalizaci√≥n que controla el equilibrio **bias‚Äìvarianza**.  
  - \( C \) grande ‚Üí menos errores, mayor varianza (overfitting).  
  - \( C \) peque√±o ‚Üí m√°s tolerancia a errores, mayor bias (underfitting).

---

## 6. SVM no lineales y el truco del kernel

Si los datos no son separables linealmente, SVM aplica una **transformaci√≥n no lineal** de las caracter√≠sticas a un espacio de mayor dimensi√≥n:

\[
\phi: \mathbb{R}^n \rightarrow \mathbb{R}^m
\]

donde en ese espacio **s√≠** existe un separador lineal.

En lugar de calcular expl√≠citamente \(\phi(x)\), SVM usa una **funci√≥n kernel** \(K(x_i, x_j)\) que mide la similitud entre puntos en ese espacio transformado: 

\[
K(x_i, x_j) = \langle \phi(x_i), \phi(x_j) \rangle
\]

en cuetion toma todo par de puntos , \(x_i, x_j\) y crea una matriz kernel , y el algoritmo de SVM usa solo esa matriz para encontrar los multiplicadores de Lagrange, y por tanto, la frontera √≥ptima.

---

## 7. Funciones de kernel m√°s usadas

| Kernel | F√≥rmula | Tipo de frontera |
|---------|----------|------------------|
| **Lineal** | \( K(x, x') = x^T x' \) | Hiperplano |
| **Polinomial** | \( K(x, x') = (x^T x' + c)^d \) | Curva polin√≥mica |
| **RBF (Radial Basis Function)** | \( K(x, x') = e^{-\gamma ||x - x'||^2} \) | Frontera no lineal |
| **Sigmoide** | \( K(x, x') = \tanh(\alpha x^T x' + c) \) | Similar a una red neuronal |

---

## 8. Interpretaci√≥n geom√©trica

- En el **espacio original**, la frontera puede ser curva o compleja.  
- En el **espacio transformado**, SVM encuentra un hiperplano lineal.  
- Los **vectores de soporte** son los √∫nicos puntos que realmente afectan el modelo.  

---

## 9. Propiedades clave

- **Margen m√°ximo:** mejor generalizaci√≥n.  
- **Dependencia en pocos puntos:** eficiente.  
- **Kernel trick:** permite separaciones no lineales sin aumentar mucho el costo computacional.

---

## 10. Ventajas y desventajas

‚úÖ **Ventajas:**
- Alta precisi√≥n en espacios de gran dimensi√≥n.  
- Robusto frente a overfitting (si \(C\) y \(\gamma\) se eligen bien).  
- Usa solo vectores de soporte ‚Üí modelo compacto.

‚ùå **Desventajas:**
- Ajuste de par√°metros \(C\) y \(\gamma\) puede ser complejo.  
- No escala bien con datasets enormes.  
- No produce probabilidades directamente (solo etiquetas).

---

## 11. Intuici√≥n visual

- **SVM lineal:** encuentra la recta/plano que maximiza el margen.  
- **SVM con kernel:** proyecta los puntos en una dimensi√≥n superior donde s√≠ existe una separaci√≥n lineal.  
- Al volver al espacio original ‚Üí frontera no lineal.

---

## 12. Resumen

| Concepto | Descripci√≥n |
|-----------|-------------|
| Tipo | Supervisado (clasificaci√≥n) |
| Hip√≥tesis | \( f(x) = w^T x + b \) |
| Optimizaci√≥n | Maximiza el margen \(\frac{2}{||w||}\) |
| Par√°metro | \(C\): control de error; \(\gamma\): forma del kernel RBF |
| Kernel Trick | Mapea los datos a un espacio donde son separables |
| Resultado | Frontera de decisi√≥n √≥ptima definida por vectores de soporte |
